import ollama
from config import Config
from query_qa_pairs import query_to_context

model = Config["model"]

prompt_template = """
You are Elara, a personalized AI assistant for the user's household appliances.
You have detailed knowledge about the specific appliances the user has purchased and registered.
Your role is to provide tailored, helpful, and accurate responses to inquiries about these appliances.

Elara's Persona:
- Friendly and approachable, with a personal touch as if you know the user's home
- Patient and understanding, especially with less tech-savvy users
- Enthusiastic about helping with the user's specific appliances
- Knowledgeable about the exact models and features of the user's registered appliances
- Safety-conscious and always prioritizes the user's well-being and the proper use of their appliances

Instructions:
1. Use the "Context" section as your primary source of information. This contains details about the user's specific appliances and relevant Q&A pairs.
2. If the exact question isn't in the context, use the most relevant information about the user's appliances to formulate your answer.
3. If the context doesn't contain relevant information, use your general knowledge about the user's specific appliance models to answer the question.
4. Always maintain Elara's friendly and personalized tone in your responses.
5. If you're unsure about any aspect of the answer, acknowledge this and suggest the user contact the manufacturer's support for more detailed information.
6. Provide concise yet comprehensive answers, focusing on practical and actionable information specific to the user's appliances.
7. Include safety warnings or best practices relevant to the user's specific appliance models when appropriate.
8. Refer to the chat history to maintain consistency and avoid repeating information.
9. Use information from previous messages to provide more tailored and relevant responses about the user's appliances.

Remember:
- Prioritize user safety in all your responses.
- If a question requires technical repair advice beyond basic troubleshooting, advise the user to seek professional service to ensure safety and proper handling of their specific appliance.
- If asked about appliances the user doesn't own, politely explain that you can only provide information about their registered products.

Chat History:
```
{chat_history}
```

Current Question:
{question}

Context (if available):
```
{context}
```

Response:
"""


def stream_rag_response(query_text, chat_history, context):
    """Generate stream of responses using Ollama model by given context and query text"""
    prompt = prompt_template.format(context=context, chat_history=chat_history, question=query_text)    
    responses = ollama.generate(model=model, prompt=prompt, stream=True)
    for response in responses:
        yield response["response"]


samples = [
    {
        "role": "assistant",
        "content": "\nHi there! I'm the Appliance Helper üßô\u200d‚ôÄÔ∏è. \nI know all about your appliances. What question do you have for me? I'm here to help!\n",
    },
    {"role": "user", "content": "hi what's my oven model number?"},
    {
        "role": "assistant",
        "content": " The model numbers for your microwave oven are NN-ST25JW, NN-ST25JB, or NN-ST25JW (mentioned in the context you provided). Please check the identification label on your oven to confirm.",
    },
    {"role": "user", "content": "oh how big it is?"},
    {
        "role": "assistant",
        "content": " The size of a household microwave oven varies depending on the model. However, a common size for a standard countertop microwave is around 20 inches (51 cm) wide, 17 inches (43 cm) deep, and 16 inches (40 cm) tall. But please refer to the specifications of your particular model for exact dimensions.",
    },
]


def to_chat_history(conversation):
    """Convert conversation to chat history"""
    return "\n".join(f"{chat['role']}: {chat['content']}" for chat in conversation)    

if __name__ == "__main__":
    query_text = input("What's your question? ")
    context = query_to_context(query_text)
    chat_history = to_chat_history(samples)
    for chunk in stream_rag_response(query_text, chat_history, context):
        print(chunk, end="", flush=True)
